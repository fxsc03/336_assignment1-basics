from curses import raw, resetty
from re import S, split
import numpy as np
from typing import Dict, List, Set, Tuple, Iterable, Iterator
import regex as re
import json
import tiktoken


# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
PAT = re.compile(r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")


# é¢„åˆ†è¯åˆ‡å‰²text
def pre_tokenization(text, special_tokens) -> List[str]:

    result: List[str] = []

    # æ²¡æœ‰special_tokensç›´æ¥åˆ‡åˆ†ç„¶åè¿”å›
    if not special_tokens:
        for m in PAT.finditer(text):
            result.append(m.group())
        return result
    
    # æ’åºé˜²æ­¢special_tokenè¢«åˆ‡é”™äº†
    sorted_special_tokens = sorted(special_tokens, key = len, reverse = True)
    parttern_string = "(" + "|".join(re.escape(t) for t in sorted_special_tokens) + ")"
    chunks = re.split(parttern_string, text)

    # æ³¨æ„ç°åœ¨çš„chunksæ˜¯æ•´ä¸ªæ–‡æœ¬,chunkæ˜¯å¥å­

    for chunk in chunks:
        if not chunk:
            continue
        if chunk in special_tokens:
            result.append(chunk)
            continue
        split_chunk = PAT.finditer(chunk)
        for s in split_chunk:
            result.append(s.group())
    
    return result

# text = "Hello, world! I'm ready."
# special_tokens = []

# output = pre_tokenization(text, special_tokens)
# print(output)

    


class Tokenizer:

    def __init__(self, vocab, merges, special_tokens = None):
        # id ->bytes
        self.vocab = vocab.copy()
        self.merges = merges 
        self.special_tokens: List[str] = special_tokens or []
        
        # æŠŠspecial_tokensä¸­çš„tokenåŠ åˆ°vocabä¸­
        next_id = 0
        if self.vocab:       
            next_id = max(self.vocab.keys()) + 1
        if special_tokens :
            for st in special_tokens:
                st_bytes = st.encode("utf-8")
                if st_bytes not in self.vocab.values():
                    self.vocab[next_id] = st_bytes
                    next_id += 1
        
        # æ–¹ä¾¿encodeçš„æ—¶å€™æŸ¥è¡¨
        self.bytes_to_id = {v: k for k, v in self.vocab.items()}

        self.bpe_ranks = {pair: i for i, pair in enumerate(self.merges)}
        
    
    @classmethod
    def from_files(cls, vocab_filepath, merge_filepath, specail_tokens: List[str] | None = None):


        vocab: dict[int, bytes] = []
        with open(vocab_filepath, "r", encoding = "utf-8")as f:
            raw_vocab = json.load(f)
        
        for k, v in raw_vocab.items():
            new_key = int(k)

            # vå¯èƒ½æ˜¯strï¼Œå¯èƒ½æ˜¯numï¼Œåšä¸åŒè½¬æ¢
            if isinstance(v, str):
                new_value = v.encode("utf-8")
            else:
                new_value = bytes[v]
            vocab[new_key] = new_value
        
        merges = []
        with open(merge_filepath, "r", encoding = "utf-8") as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œæˆ–æ³¨é‡Š
                if not line or line.startswith("#"):
                    continue
                
                parts = line.split()

                if len(parts) == 2:
                    # è§£æ "117,118" -> [117, 118] -> b'uv'
                    p0 = bytes(map(int, parts[0].split(',')))
                    p1 = bytes(map(int, parts[1].split(',')))
                    merges.append((p0, p1))
            
        return cls(vocab, merges, specail_tokens)



    def encode(self, text: str) -> List[int]:
        if not text:
            return []
        
        # ç°åœ¨é‡Œé¢æœ‰å•è¯å’Œspecial_token
        chunks = pre_tokenization(text, self.special_tokens)

        final_ids:List[int] = []
        for chunk in chunks:
            # å…ˆå¤„ç†ç‰¹æ®Šå­—ç¬¦
            if chunk in self.special_tokens:
                chunk_bytes = chunk.encode("utf-8")
                if chunk_bytes in self.bytes_to_id:
                    final_ids.append(self.bytes_to_id[chunk_bytes])
                else:
                    raise ValueError(f"ç‰¹æ®Šç¬¦å· '{chunk}' è¢«è¯†åˆ«åˆ°äº†ï¼Œä½†åœ¨è¯è¡¨ä¸­æ‰¾ä¸åˆ°å¯¹åº”çš„ ID\n")
                continue
            
            word_bytes = chunk.encode("utf-8")

            # æŠŠæ¯ä¸€ä¸ªå­—æ¯æ‹¿å‡ºæ¥ç”¨äºæŸ¥mergeè¡¨åˆå¹¶
            parts = [bytes([b]) for b in word_bytes]
            
            while True:
                # æ¯æ¬¡æ ¹æ®vocabè¡¨æŸ¥ä¸€å¯¹åˆ
                if len(parts) < 2:
                    break
                
                # å…ˆéå†æ‰¾å‡ºèƒ½å¤Ÿåˆå¹¶çš„ä¸´å¯¹ï¼Œä¹Ÿå°±æ˜¯bpe_ranksä¸­æœ€å°çš„
                pairs = [(parts[i], parts[i+1]) for i in range(len(parts)-1)]
                best_pairs = min(pairs, key =lambda pair: self.bpe_ranks.get(pair, float('inf')) )

                if best_pairs not in self.bpe_ranks:
                    break

                # å¼€å§‹merge
                new_parts = []
                i = 0
                p0, p1 = best_pairs
                target = p0 + p1
                while i < len(parts):
                    if i< len(parts) -1 and parts[i] == p0 and parts[i+1] == p1:
                        new_parts.append(target)
                        i += 2
                    else:
                        new_parts.append(parts[i])
                        i += 1
                parts = new_parts

            # æ›´æ–°final_list 
            for part in parts:
                if part in self.bytes_to_id:
                    final_ids.append(self.bytes_to_id[part])

        return final_ids


    # streamingæµå¼å¤„ç†
    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:
        for text in iterable:
            yield from self.encode(text)

    # bytes->unicode->str
    def decode(self, ids: List[int]) -> str:
        byte_data = b"".join(self.vocab[i] for i in ids)
        text = byte_data.decode("utf-8", errors = 'replace')
        return text
        

# # ==========================================
# # ç®€å•çš„æ‰‹åŠ¨æµ‹è¯•è„šæœ¬
# # ==========================================
# if __name__ == "__main__":
#     # 1. ä¼ªé€ æ•°æ®
#     # æˆ‘ä»¬æ‰‹åŠ¨æ„é€ ä¸€ä¸ª vocabï¼Œå‡è£…è¿™äº›æ˜¯è®­ç»ƒå¥½çš„
#     # æ³¨æ„ï¼šè¿™é‡Œç‰¹æ„æŠŠä¸­æ–‡ "ä½ " (b'\xe4\xbd\xa0') æ‹†æˆäº†ä¸‰ä¸ªç¢ç‰‡ï¼Œæµ‹è¯•æ‹¼æ¥èƒ½åŠ›
#     fake_vocab = {
#         0: b'H',
#         1: b'el',
#         2: b'lo',
#         3: b', ',
#         4: b'World',
#         5: b'!',
#         6: b'\xe4', # 'ä½ ' çš„ç¬¬1ä¸ªå­—èŠ‚
#         7: b'\xbd', # 'ä½ ' çš„ç¬¬2ä¸ªå­—èŠ‚
#         8: b'\xa0'  # 'ä½ ' çš„ç¬¬3ä¸ªå­—èŠ‚
#     }
    
#     # decode ä¸éœ€è¦ mergesï¼Œç»™ä¸ªç©ºåˆ—è¡¨å°±è¡Œ
#     fake_merges = [] 
    
#     # 2. å®ä¾‹åŒ–ä½ çš„ Tokenizer
#     # æ³¨æ„ï¼šè¿™é‡Œä¼šè§¦å‘ä½ çš„ __init__ï¼Œç¡®ä¿ä½ ä¹‹å‰çš„ __init__ ä»£ç æ˜¯å¥½çš„
#     tokenizer = Tokenizer(fake_vocab, fake_merges, special_tokens=None)
    
#     # 3. å‡†å¤‡æµ‹è¯• ID åºåˆ—
#     # å¯¹åº”: H + el + lo + , + World + ! + (ä½ )
#     test_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8]
    
#     # 4. è¿è¡Œè§£ç 
#     print(f"è¾“å…¥ ID: {test_ids}")
#     try:
#         decoded_text = tokenizer.decode(test_ids)
#         print(f"è§£ç ç»“æœ: {decoded_text}")
        
#         # 5. éªŒè¯æ­£ç¡®æ€§
#         expected_text = "Hello, World!ä½ "
#         if decoded_text == expected_text:
#             print("âœ… æµ‹è¯•é€šè¿‡ï¼")
#         else:
#             print(f"âŒ æµ‹è¯•å¤±è´¥ã€‚\né¢„æœŸ: {expected_text}\nå®é™…: {decoded_text}")
            
#     except Exception as e:
#         print(f"âŒ è¿è¡ŒæŠ¥é”™: {e}")


# ==========================================
# æ·±åº¦é€»è¾‘æµ‹è¯•è„šæœ¬
# ==========================================
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯• Encode æ¨¡å—...")

    # 1. æ„é€ ä¸€ä¸ªå¾®å‹è¯è¡¨
    # åŒ…å«äº†åŸºç¡€å­—æ¯ã€éƒ¨åˆ†åˆå¹¶è¯ã€ä»¥åŠç‰¹æ®Šç¬¦å·
    fake_vocab = {
        # --- åŸºç¡€å­—èŠ‚ ---
        0: b'u', 1: b'n', 2: b'i', 3: b'g', 4: b'h', 5: b't', 
        6: b'a', 7: b'b',
        # --- BPE åˆå¹¶äº§ç”Ÿçš„è¯ ---
        8: b'un',   # u + n
        9: b'ni',   # n + i (ç”¨æ¥æµ‹è¯•ä¼˜å…ˆçº§çš„å¹²æ‰°é¡¹)
        10: b'uni', # un + i
        11: b'gh',  # g + h
        12: b'ght', # gh + t
        # --- ç‰¹æ®Šç¬¦å· ---
        13: b'<|END|>'
    }

    # 2. æ„é€ åˆå¹¶è§„åˆ™ (æ³¨æ„é¡ºåºï¼ä¸‹æ ‡è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜)
    # é€»è¾‘é™·é˜±ï¼šæˆ‘ä»¬åŒæ—¶æœ‰ (u, n) å’Œ (n, i)ã€‚
    # å¯¹äºå•è¯ "uni"ï¼š
    # - å¦‚æœå…ˆåˆå¹¶ (u, n)ï¼Œå˜æˆ un, i -> å†åˆå¹¶ (un, i) -> uni (æ­£ç¡®è·¯å¾„)
    # - å¦‚æœå…ˆåˆå¹¶ (n, i)ï¼Œå˜æˆ u, ni -> æ— æ³•åˆå¹¶æˆ uni (å› ä¸ºæ²¡æœ‰ u+ni çš„è§„åˆ™)
    fake_merges = [
        (b'u', b'n'),   # Rank 0 (æœ€é«˜ä¼˜å…ˆçº§)
        (b'g', b'h'),   # Rank 1
        (b'un', b'i'),  # Rank 2
        (b'gh', b't'),  # Rank 3
        (b'n', b'i'),   # Rank 4 (ä¼˜å…ˆçº§ä½ï¼Œé™·é˜±ï¼)
    ]
    
    special_tokens = ["<|END|>"]

    # 3. åˆå§‹åŒ–
    tokenizer = Tokenizer(fake_vocab, fake_merges, special_tokens)
    
    # 4. æµ‹è¯•æ¡ˆä¾‹
    # ç›®æ ‡æ–‡æœ¬: "unight<|END|>"
    # é¢„æœŸæ‹†è§£:
    #   "unight" -> b'u', b'n', b'i', b'g', b'h', b't'
    #   Step 1: (u, n) åˆå¹¶ -> [un, i, g, h, t]
    #   Step 2: (g, h) åˆå¹¶ -> [un, i, gh, t]
    #   Step 3: (un, i) åˆå¹¶ -> [uni, gh, t]
    #   Step 4: (gh, t) åˆå¹¶ -> [uni, ght]
    #   Step 5: <|END|> ç›´æ¥æŸ¥è¡¨
    # æœ€ç»ˆ ID: [10 (uni), 12 (ght), 13 (<|END|>)]
    text = "unight<|END|>"
    
    try:
        print(f"\næµ‹è¯•æ–‡æœ¬: '{text}'")
        ids = tokenizer.encode(text)
        print(f"Encode ç»“æœ: {ids}")
        
        # éªŒè¯ Encode
        expected_ids = [10, 12, 13]
        if ids == expected_ids:
            print("âœ… Encode é€»è¾‘æ­£ç¡®ï¼(ä¼˜å…ˆçº§å¤„ç†å®Œç¾)")
        else:
            print(f"âŒ Encode å¤±è´¥ã€‚\né¢„æœŸ: {expected_ids}\nå®é™…: {ids}")
            # å¦‚æœä½ è¾“å‡ºäº† [0, 9, ... ] è¯´æ˜ä¼˜å…ˆçº§æé”™äº†ï¼Œå…ˆåˆå¹¶äº† ni
            
        # éªŒè¯ Decode (Round Trip)
        decoded_text = tokenizer.decode(ids)
        print(f"Decode ç»“æœ: '{decoded_text}'")
        
        if decoded_text == text:
            print("âœ… Decode è¿˜åŸæ— æŸï¼")
        else:
            print(f"âŒ Decode è¿˜åŸå¤±è´¥ã€‚\né¢„æœŸ: '{text}'\nå®é™…: '{decoded_text}'")

    except Exception as e:
        import traceback
        print(f"âŒ ç¨‹åºå´©æºƒ: {e}")
        traceback.print_exc()